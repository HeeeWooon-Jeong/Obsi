회귀(Regression) 문제를 해결하는 데 포함되는 주요 최적화 기법 중 하나입니다. 
회귀 문제는 종속 변수와 독립 변수 간의 관계를 모델링하고, 
주어진 데이터에 대한 예측을 수행하는 것을 목표로 합니다. 
이 때, 경사 하강법은 회귀 모델의 파라미터(계수)를 최적화하기 위한 방법 중 하나로 활용됩니다.

---

![](https://i.imgur.com/Vt67DhT.png)
	잔차 제곱의 합 = 선형회귀에서 쓰던 **y= mx + b**
	최소 제곱법     =  잔차제곱의 합을 최소화 하는 방법 -- 노이즈에 취약하다는 단점!

![](https://i.imgur.com/ycLbCnT.png)
이상치에 취약함  -- 최소 제곱법

---
위 문제를 해결하기 위한게 --> 경사 하강법

![](https://i.imgur.com/3WE4FwQ.png)
수렴

![](https://i.imgur.com/e6tWxsb.png)
발산

보폭이 작으면 여러번 수행해야하고
보폭이 크면 목표지점을 찾아 가기가 어렵다  

수렴 / 발산
== 학습률

![](https://i.imgur.com/Y7ulzzc.png)
일반적으로 아래의 값들을 많이 사용함

---

경사하강법에서  최적의 파라미터를 찾기 위해 훈련세트에 있는 모든데이터를 한번씩 다 사용하는것을
== Epoch에포크

경사하강법은 모든데이터를 사용하기 때문에 컴퓨터 자원을 많이 필요로 하고 
시간도 오래걸릴 수 있음

--> 이것을 보완한것이

![](https://i.imgur.com/S0NeqgT.png)

매 단계마다 딱 하나의 데이터만 지정해서 기울기를 계산하는 작업 수행 
 
SGD - 파라미터로 사용 optimizer
## [경사하강법을 gif 로 보기](https://alykhantejani.github.io/images/gradient_descent_line_graph.gif)




---
# [[미니 배치 경사 하강법 (Mini-Batch Gradient Descent)]]


미니 배치 경사 하강법은 일반적으로 SGD와 경사 하강법 사이의 절충안으로 사용됩니다. 즉, 미니 배치 크기를 설정하여 데이터 일부만 사용하며 모델을 학습합니다. 이렇게 하면 더 안정적인 학습과 빠른 수렴을 조절할 수 있습니다.

따라서 선택은 다음을 고려하여 이루어져야 합니다:

- 데이터의 규모: 대규모 데이터셋에는 SGD 또는 미니 배치 경사 하강법이 일반적으로 더 적합합니다.
- 학습 시간: SGD는 빠르게 학습하지만 무작위성으로 인해 수렴 속도가 다를 수 있습니다.
- 메모리 제한: 메모리 제한이 있는 경우, 미니 배치 경사 하강법이 유용합니다.
- 모델의 일반화: SGD는 지역 최소값에서 벗어나기 위한 무작위성을 가지므로 모델의 일반화를 향상시킬 수 있습니다.


---

![[Pasted image 20231004133307.png]]
****
---
